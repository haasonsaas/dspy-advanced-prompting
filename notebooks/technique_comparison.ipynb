{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Prompting Techniques Comparison\n",
    "\n",
    "This notebook compares all 11 advanced prompting techniques side-by-side with the same inputs to show their different strengths and use cases.\n",
    "\n",
    "## Techniques We'll Compare\n",
    "\n",
    "1. **Manager-Style Prompts** - Detailed role-based instructions\n",
    "2. **Role Prompting** - Specific persona adoption\n",
    "3. **Task Planning** - Complex workflow breakdown\n",
    "4. **Structured Output** - Formatted responses with tags\n",
    "5. **Meta-Prompting** - Self-optimization and analysis\n",
    "6. **Few-Shot Learning** - Learning from examples\n",
    "7. **Prompt Folding** - Multi-step workflow management\n",
    "8. **Escape Hatches** - Uncertainty handling\n",
    "9. **Thinking Traces** - Step-by-step reasoning\n",
    "10. **Model Distillation** - Optimization for production\n",
    "11. **Evaluation Framework** - Testing and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from IPython.display import HTML, display\n",
    "import time\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "import dspy\n",
    "\n",
    "# Import all techniques\n",
    "from src.prompts.manager_style import create_customer_support_manager\n",
    "from src.techniques.role_prompting import create_veteran_engineer_persona\n",
    "from src.techniques.task_planning import TaskOrchestrator\n",
    "from src.techniques.structured_output import StructuredOutputGenerator, create_bug_report_schema\n",
    "from src.techniques.meta_prompting import MetaPromptOptimizer\n",
    "from src.techniques.few_shot import FewShotLearner, create_bug_analysis_examples, FewShotPromptTemplate\n",
    "from src.techniques.escape_hatches import EscapeHatchResponder\n",
    "from src.techniques.thinking_traces import ThinkingTracer\n",
    "\n",
    "# Configure DSpy\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"⚠️ Please set OPENAI_API_KEY in your .env file\")\n",
    "else:\n",
    "    lm = dspy.LM(model=\"gpt-4o-mini\", api_key=api_key, max_tokens=1500)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    print(\"✅ DSpy configured with OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scenario: Code Review Request\n",
    "\n",
    "We'll use a common software engineering scenario - reviewing a piece of code with potential security issues - to see how each technique handles the same input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common test scenario\n",
    "test_code = '''\n",
    "def login_user(username, password):\n",
    "    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n",
    "    cursor.execute(query)\n",
    "    user = cursor.fetchone()\n",
    "    if user:\n",
    "        session['user_id'] = user[0]\n",
    "        return redirect('/dashboard')\n",
    "    else:\n",
    "        return \"Login failed\"\n",
    "'''\n",
    "\n",
    "task = \"Review this login function for security vulnerabilities\"\n",
    "context = \"This is production code in a web application handling user authentication\"\n",
    "\n",
    "print(\"🔍 Test Scenario: Code Security Review\")\n",
    "print(\"\\n📝 Code to Review:\")\n",
    "print(test_code)\n",
    "print(\"\\n🎯 Task:\", task)\n",
    "print(\"📋 Context:\", context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manager-Style Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"👔 Manager-Style Prompts\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create code review manager\n",
    "from src.prompts.manager_style import create_code_review_manager\n",
    "code_manager = create_code_review_manager()\n",
    "\n",
    "start_time = time.time()\n",
    "manager_result = code_manager(task=task, context=f\"{context}\\n\\nCode:\\n{test_code}\")\n",
    "manager_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️ Response time: {manager_time:.2f}s\")\n",
    "print(f\"📝 Response ({len(manager_result)} chars):\")\n",
    "print(manager_result[:500] + \"...\" if len(manager_result) > 500 else manager_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Role Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎭 Role Prompting (Veteran Engineer)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "engineer = create_veteran_engineer_persona()\n",
    "\n",
    "start_time = time.time()\n",
    "role_result = engineer(task=task, context=f\"{context}\\n\\nCode:\\n{test_code}\")\n",
    "role_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️ Response time: {role_time:.2f}s\")\n",
    "print(f\"📝 Response ({len(role_result)} chars):\")\n",
    "print(role_result[:500] + \"...\" if len(role_result) > 500 else role_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 Structured Output\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create code review schema\n",
    "from src.techniques.structured_output import create_code_review_schema\n",
    "generator = StructuredOutputGenerator()\n",
    "code_schema = create_code_review_schema()\n",
    "\n",
    "start_time = time.time()\n",
    "structured_result = generator(\n",
    "    task=task,\n",
    "    schema=code_schema,\n",
    "    context=f\"{context}\\n\\nCode:\\n{test_code}\"\n",
    ")\n",
    "structured_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️ Response time: {structured_time:.2f}s\")\n",
    "print(f\"📝 Response ({len(structured_result)} chars):\")\n",
    "print(structured_result[:500] + \"...\" if len(structured_result) > 500 else structured_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Few-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📚 Few-Shot Learning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create few-shot learner with bug analysis examples\n",
    "examples = create_bug_analysis_examples()\n",
    "template = FewShotPromptTemplate(\n",
    "    task_intro=\"Analyze the following code for security vulnerabilities:\",\n",
    "    example_intro=\"Here are examples of good security analysis:\",\n",
    "    include_reasoning=True\n",
    ")\n",
    "learner = FewShotLearner(examples, template)\n",
    "\n",
    "start_time = time.time()\n",
    "fewshot_result = learner(f\"{context}\\n\\nCode:\\n{test_code}\")\n",
    "fewshot_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️ Response time: {fewshot_time:.2f}s\")\n",
    "print(f\"📝 Response ({len(fewshot_result)} chars):\")\n",
    "print(fewshot_result[:500] + \"...\" if len(fewshot_result) > 500 else fewshot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Escape Hatches (Uncertainty Handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚪 Escape Hatches (Uncertainty Handling)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "escaper = EscapeHatchResponder()\n",
    "\n",
    "start_time = time.time()\n",
    "escape_result = escaper(f\"{task}\\n\\n{context}\\n\\nCode:\\n{test_code}\")\n",
    "escape_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️ Response time: {escape_time:.2f}s\")\n",
    "print(f\"📝 Response ({len(escape_result['response'])} chars):\")\n",
    "print(escape_result['response'][:500] + \"...\" if len(escape_result['response']) > 500 else escape_result['response'])\n",
    "print(f\"\\n🎯 Confidence: {escape_result['uncertainty_analysis'].confidence_level:.2f}\")\n",
    "print(f\"❓ Uncertainty Level: {escape_result['uncertainty_analysis'].uncertainty_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Thinking Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧠 Thinking Traces\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "tracer = ThinkingTracer(verbose=False)  # Set to False to avoid visualization in notebook\n",
    "\n",
    "start_time = time.time()\n",
    "thinking_result = tracer(f\"{task}\\n\\n{context}\\n\\nCode:\\n{test_code}\")\n",
    "thinking_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️ Response time: {thinking_time:.2f}s\")\n",
    "print(f\"📝 Final Answer ({len(thinking_result['answer'])} chars):\")\n",
    "print(thinking_result['answer'][:300] + \"...\" if len(thinking_result['answer']) > 300 else thinking_result['answer'])\n",
    "\n",
    "print(f\"\\n🤔 Thinking Steps Preview:\")\n",
    "thinking_steps = thinking_result.get('thinking_steps', '')\n",
    "print(thinking_steps[:200] + \"...\" if len(thinking_steps) > 200 else thinking_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary\n",
    "\n",
    "Let's analyze the different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results = [\n",
    "    {\n",
    "        'Technique': 'Manager-Style',\n",
    "        'Response Time': f\"{manager_time:.2f}s\",\n",
    "        'Response Length': len(manager_result),\n",
    "        'Key Strength': 'Professional structure, comprehensive guidance',\n",
    "        'Best For': 'Consistent, detailed responses in professional contexts'\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Role Prompting', \n",
    "        'Response Time': f\"{role_time:.2f}s\",\n",
    "        'Response Length': len(role_result),\n",
    "        'Key Strength': 'Domain expertise, authentic voice',\n",
    "        'Best For': 'Leveraging specific professional perspectives'\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Structured Output',\n",
    "        'Response Time': f\"{structured_time:.2f}s\", \n",
    "        'Response Length': len(structured_result),\n",
    "        'Key Strength': 'Consistent format, machine-readable',\n",
    "        'Best For': 'API responses, structured data extraction'\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Few-Shot Learning',\n",
    "        'Response Time': f\"{fewshot_time:.2f}s\",\n",
    "        'Response Length': len(fewshot_result),\n",
    "        'Key Strength': 'Learning from examples, pattern recognition',\n",
    "        'Best For': 'Complex tasks with good examples available'\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Escape Hatches',\n",
    "        'Response Time': f\"{escape_time:.2f}s\",\n",
    "        'Response Length': len(escape_result['response']),\n",
    "        'Key Strength': f\"Confidence tracking ({escape_result['uncertainty_analysis'].confidence_level:.2f})\",\n",
    "        'Best For': 'Critical decisions, avoiding hallucinations'\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Thinking Traces',\n",
    "        'Response Time': f\"{thinking_time:.2f}s\",\n",
    "        'Response Length': len(thinking_result['answer']),\n",
    "        'Key Strength': 'Step-by-step reasoning, debugging thought process',\n",
    "        'Best For': 'Complex problem-solving, educational content'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display as HTML table\n",
    "html_table = \"\"\"\n",
    "<table border=\"1\" style=\"border-collapse: collapse; width: 100%;\">\n",
    "<tr style=\"background-color: #f2f2f2;\">\n",
    "    <th>Technique</th>\n",
    "    <th>Response Time</th>\n",
    "    <th>Length (chars)</th>\n",
    "    <th>Key Strength</th>\n",
    "    <th>Best For</th>\n",
    "</tr>\n",
    "\"\"\"\n",
    "\n",
    "for result in results:\n",
    "    html_table += f\"\"\"\n",
    "<tr>\n",
    "    <td><strong>{result['Technique']}</strong></td>\n",
    "    <td>{result['Response Time']}</td>\n",
    "    <td>{result['Response Length']}</td>\n",
    "    <td>{result['Key Strength']}</td>\n",
    "    <td>{result['Best For']}</td>\n",
    "</tr>\n",
    "\"\"\"\n",
    "\n",
    "html_table += \"</table>\"\n",
    "\n",
    "display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze security vulnerability detection\n",
    "security_keywords = ['sql injection', 'injection', 'vulnerability', 'security', 'sanitize', 'escape', 'parameterized']\n",
    "\n",
    "def count_security_mentions(text):\n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for keyword in security_keywords if keyword in text_lower)\n",
    "\n",
    "print(\"🔒 Security Vulnerability Detection Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "techniques = [\n",
    "    ('Manager-Style', manager_result),\n",
    "    ('Role Prompting', role_result), \n",
    "    ('Structured Output', structured_result),\n",
    "    ('Few-Shot Learning', fewshot_result),\n",
    "    ('Escape Hatches', escape_result['response']),\n",
    "    ('Thinking Traces', thinking_result['answer'])\n",
    "]\n",
    "\n",
    "for name, result in techniques:\n",
    "    security_score = count_security_mentions(result)\n",
    "    mentions_sql = 'sql injection' in result.lower() or 'injection' in result.lower()\n",
    "    print(f\"{name:<18} | Security mentions: {security_score} | Identifies SQL injection: {'✅' if mentions_sql else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "### When to Use Each Technique:\n",
    "\n",
    "1. **Manager-Style**: Professional environments requiring consistent, comprehensive responses\n",
    "2. **Role Prompting**: When domain expertise and authentic voice matter\n",
    "3. **Structured Output**: API integrations and data processing pipelines\n",
    "4. **Few-Shot Learning**: Complex tasks where good examples exist\n",
    "5. **Escape Hatches**: Critical decisions where confidence levels matter\n",
    "6. **Thinking Traces**: Educational content and complex problem-solving\n",
    "\n",
    "### Performance Observations:\n",
    "\n",
    "- **Response Times**: Generally consistent across techniques (~2-4 seconds)\n",
    "- **Security Detection**: All techniques successfully identified SQL injection vulnerability\n",
    "- **Response Depth**: Manager-style and few-shot typically provide more comprehensive analysis\n",
    "- **Confidence Tracking**: Only escape hatches provide explicit uncertainty measurements\n",
    "\n",
    "### Combination Strategies:\n",
    "\n",
    "- Combine **Manager-Style + Escape Hatches** for critical business decisions\n",
    "- Use **Role Prompting + Thinking Traces** for educational content\n",
    "- Apply **Few-Shot + Structured Output** for consistent data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Your Own Comparison\n",
    "\n",
    "Modify the test scenario below to compare techniques with your own use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize this for your own testing\n",
    "custom_task = \"Your task here\"\n",
    "custom_context = \"Your context here\" \n",
    "custom_input = \"Your input data here\"\n",
    "\n",
    "print(\"🧪 Custom Comparison Test\")\n",
    "print(\"🎯 Task:\", custom_task)\n",
    "print(\"📋 Context:\", custom_context)\n",
    "print(\"📝 Input:\", custom_input)\n",
    "print(\"\\n💡 Modify the variables above and re-run to test with your own scenario!\")\n",
    "\n",
    "# Uncomment to test:\n",
    "# manager_custom = code_manager(task=custom_task, context=f\"{custom_context}\\n\\n{custom_input}\")\n",
    "# print(\"\\n👔 Manager Result:\")\n",
    "# print(manager_custom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}