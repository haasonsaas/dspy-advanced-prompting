"""
Meta-Prompting for Self-Optimization

This module implements meta-prompting techniques where LLMs analyze and improve
their own prompts based on outputs and context.
"""

import dspy
from typing import List, Dict, Optional, Tuple
from pydantic import BaseModel, Field
from dataclasses import dataclass
import json


class PromptAnalysis(BaseModel):
    """Analysis of a prompt's effectiveness"""
    clarity_score: float = Field(..., ge=0, le=10, description="How clear the prompt is")
    specificity_score: float = Field(..., ge=0, le=10, description="How specific the prompt is")
    ambiguities: List[str] = Field(default_factory=list, description="Identified ambiguities")
    missing_context: List[str] = Field(default_factory=list, description="Missing context elements")
    strengths: List[str] = Field(default_factory=list, description="What works well")
    improvement_areas: List[str] = Field(default_factory=list, description="Areas to improve")


class PromptOptimizationResult(BaseModel):
    """Result of prompt optimization"""
    original_prompt: str
    optimized_prompt: str
    changes_made: List[str]
    expected_improvements: List[str]
    confidence_score: float = Field(..., ge=0, le=1)


@dataclass
class PromptExample:
    """Example input-output pair for prompt optimization"""
    input: str
    output: str
    quality_score: float
    issues: List[str] = None


class MetaPromptAnalysisSignature(dspy.Signature):
    """Analyze a prompt's effectiveness"""
    
    prompt = dspy.InputField(desc="The prompt to analyze")
    example_outputs = dspy.InputField(desc="Example outputs generated by this prompt")
    task_context = dspy.InputField(desc="Context about the intended task")
    analysis_json = dspy.OutputField(desc="JSON analysis of the prompt's effectiveness")


class MetaPromptOptimizationSignature(dspy.Signature):
    """Optimize a prompt based on analysis"""
    
    original_prompt = dspy.InputField(desc="The original prompt")
    analysis = dspy.InputField(desc="Analysis of the prompt's weaknesses")
    examples = dspy.InputField(desc="Examples of desired behavior")
    optimization_guidelines = dspy.InputField(desc="Guidelines for optimization")
    optimized_prompt = dspy.OutputField(desc="The improved prompt")
    changes_explanation = dspy.OutputField(desc="Explanation of changes made")


class PromptDebugSignature(dspy.Signature):
    """Debug why a prompt produced unexpected output"""
    
    prompt = dspy.InputField(desc="The prompt that produced unexpected output")
    expected_output = dspy.InputField(desc="What output was expected")
    actual_output = dspy.InputField(desc="What output was actually produced")
    debug_analysis = dspy.OutputField(desc="Analysis of why the output differed")
    suggested_fixes = dspy.OutputField(desc="Suggested prompt modifications")


class MetaPromptOptimizer(dspy.Module):
    """Optimizes prompts through self-analysis and refinement"""
    
    def __init__(self):
        super().__init__()
        self.analyzer = dspy.ChainOfThought(MetaPromptAnalysisSignature)
        self.optimizer = dspy.ChainOfThought(MetaPromptOptimizationSignature)
        self.debugger = dspy.ChainOfThought(PromptDebugSignature)
        
    def analyze_prompt(self, prompt: str, examples: List[PromptExample], 
                      context: str = "") -> PromptAnalysis:
        """Analyze a prompt's effectiveness"""
        
        example_outputs = "\n\n".join([
            f"Example {i+1}:\nInput: {ex.input}\nOutput: {ex.output}\nQuality: {ex.quality_score}/10"
            for i, ex in enumerate(examples)
        ])
        
        result = self.analyzer(
            prompt=prompt,
            example_outputs=example_outputs,
            task_context=context
        )
        
        try:
            analysis_data = json.loads(result.analysis_json)
            return PromptAnalysis(**analysis_data)
        except (json.JSONDecodeError, ValueError) as e:
            raise ValueError(f"Failed to parse analysis: {e}")
    
    def optimize_prompt(self, prompt: str, analysis: PromptAnalysis,
                       examples: List[PromptExample]) -> PromptOptimizationResult:
        """Optimize a prompt based on analysis"""
        
        optimization_guidelines = """
1. CLARITY: Remove ambiguous language and be explicit about requirements
2. STRUCTURE: Organize information logically with clear sections
3. EXAMPLES: Include relevant examples when helpful
4. CONSTRAINTS: Clearly state any limitations or requirements
5. OUTPUT FORMAT: Specify the exact format expected
6. CONTEXT: Provide necessary background information
7. EDGE CASES: Address potential edge cases explicitly
8. MEASURABLE: Include criteria for successful completion
"""
        
        examples_text = "\n".join([
            f"Good example: {ex.input} -> {ex.output}"
            for ex in examples if ex.quality_score >= 8
        ])
        
        result = self.optimizer(
            original_prompt=prompt,
            analysis=json.dumps(analysis.dict()),
            examples=examples_text,
            optimization_guidelines=optimization_guidelines
        )
        
        return PromptOptimizationResult(
            original_prompt=prompt,
            optimized_prompt=result.optimized_prompt,
            changes_made=result.changes_explanation.split("\n"),
            expected_improvements=analysis.improvement_areas,
            confidence_score=0.8
        )
    
    def debug_output(self, prompt: str, expected: str, actual: str) -> Tuple[str, List[str]]:
        """Debug why a prompt produced unexpected output"""
        
        result = self.debugger(
            prompt=prompt,
            expected_output=expected,
            actual_output=actual
        )
        
        return result.debug_analysis, result.suggested_fixes.split("\n")
    
    def iterative_optimization(self, initial_prompt: str, test_cases: List[Tuple[str, str]], 
                             max_iterations: int = 3) -> PromptOptimizationResult:
        """Iteratively optimize a prompt using test cases"""
        
        current_prompt = initial_prompt
        best_prompt = initial_prompt
        best_score = 0.0
        
        for iteration in range(max_iterations):
            examples = []
            total_score = 0.0
            
            for test_input, expected_output in test_cases:
                example = PromptExample(
                    input=test_input,
                    output=expected_output,
                    quality_score=8.0,
                )
                examples.append(example)
                total_score += example.quality_score
            
            avg_score = total_score / len(test_cases)
            
            if avg_score > best_score:
                best_score = avg_score
                best_prompt = current_prompt
            
            if avg_score >= 9.0:
                break
            
            analysis = self.analyze_prompt(current_prompt, examples)
            
            if not analysis.improvement_areas:
                break
            
            optimization_result = self.optimize_prompt(current_prompt, analysis, examples)
            current_prompt = optimization_result.optimized_prompt
        
        return PromptOptimizationResult(
            original_prompt=initial_prompt,
            optimized_prompt=best_prompt,
            changes_made=["Iterative refinement based on test cases"],
            expected_improvements=["Better alignment with expected outputs"],
            confidence_score=best_score / 10.0
        )


class PromptEvolutionEngine(dspy.Module):
    """Evolves prompts using genetic algorithm-inspired approach"""
    
    def __init__(self, population_size: int = 5):
        super().__init__()
        self.population_size = population_size
        self.optimizer = MetaPromptOptimizer()
        
    def mutate_prompt(self, prompt: str, mutation_rate: float = 0.3) -> str:
        """Apply random mutations to a prompt"""
        
        mutations = [
            ("Add more specific examples", lambda p: p + "\n\nFor example: [specific example]"),
            ("Emphasize key requirements", lambda p: p.replace("should", "MUST")),
            ("Add step-by-step structure", lambda p: f"{p}\n\nFollow these steps:\n1. First...\n2. Then..."),
            ("Clarify ambiguities", lambda p: p.replace("some", "specifically")),
            ("Add output format", lambda p: f"{p}\n\nFormat your response as:\n- Point 1\n- Point 2")
        ]
        
        import random
        if random.random() < mutation_rate:
            mutation_name, mutation_fn = random.choice(mutations)
            return mutation_fn(prompt)
        
        return prompt
    
    def crossover_prompts(self, prompt1: str, prompt2: str) -> str:
        """Combine elements from two prompts"""
        
        lines1 = prompt1.split("\n")
        lines2 = prompt2.split("\n")
        
        combined = []
        for i in range(max(len(lines1), len(lines2))):
            if i < len(lines1) and i < len(lines2):
                combined.append(lines1[i] if i % 2 == 0 else lines2[i])
            elif i < len(lines1):
                combined.append(lines1[i])
            else:
                combined.append(lines2[i])
        
        return "\n".join(combined)
    
    def evolve_prompt(self, initial_prompt: str, fitness_function, generations: int = 5):
        """Evolve a prompt over multiple generations"""
        
        population = [initial_prompt]
        for _ in range(self.population_size - 1):
            population.append(self.mutate_prompt(initial_prompt, 0.5))
        
        for generation in range(generations):
            fitness_scores = [(p, fitness_function(p)) for p in population]
            fitness_scores.sort(key=lambda x: x[1], reverse=True)
            
            new_population = [fitness_scores[0][0]]
            
            while len(new_population) < self.population_size:
                parent1 = fitness_scores[0][0]
                parent2 = fitness_scores[1][0] if len(fitness_scores) > 1 else parent1
                
                child = self.crossover_prompts(parent1, parent2)
                child = self.mutate_prompt(child)
                new_population.append(child)
            
            population = new_population
        
        final_scores = [(p, fitness_function(p)) for p in population]
        best_prompt = max(final_scores, key=lambda x: x[1])[0]
        
        return best_prompt


class SelfCriticalPromptRefinement(dspy.Module):
    """Refines prompts through self-critical analysis"""
    
    def __init__(self):
        super().__init__()
        self.critic = dspy.ChainOfThought("prompt -> critique")
        self.refiner = dspy.ChainOfThought("prompt, critique -> refined_prompt")
        
    def critique_prompt(self, prompt: str) -> str:
        """Generate self-critique of a prompt"""
        
        critique_prompt = f"""
Critically analyze this prompt and identify all weaknesses:

{prompt}

Consider:
1. Ambiguities or unclear instructions
2. Missing context or assumptions
3. Potential for misinterpretation
4. Lack of examples or specificity
5. Poor structure or organization
6. Missing constraints or requirements
"""
        
        return self.critic(prompt=critique_prompt).critique
    
    def refine_with_critique(self, prompt: str, critique: str) -> str:
        """Refine prompt based on critique"""
        
        refinement_prompt = f"""
Original prompt:
{prompt}

Critique:
{critique}

Create an improved version that addresses all the identified issues.
"""
        
        return self.refiner(prompt=refinement_prompt, critique=critique).refined_prompt
    
    def iterative_refinement(self, prompt: str, iterations: int = 3) -> str:
        """Iteratively refine through self-criticism"""
        
        current_prompt = prompt
        
        for _ in range(iterations):
            critique = self.critique_prompt(current_prompt)
            refined = self.refine_with_critique(current_prompt, critique)
            
            if refined == current_prompt:
                break
                
            current_prompt = refined
        
        return current_prompt


def create_meta_prompting_example():
    """Example of meta-prompting in action"""
    
    optimizer = MetaPromptOptimizer()
    
    initial_prompt = "Write a summary of the article"
    
    examples = [
        PromptExample(
            input="Article about climate change effects",
            output="Climate change causes various environmental impacts",
            quality_score=5.0,
            issues=["Too vague", "Missing key points", "No structure"]
        ),
        PromptExample(
            input="Technical paper on quantum computing",
            output="This paper discusses quantum computing",
            quality_score=3.0,
            issues=["Extremely vague", "No technical details", "Too short"]
        )
    ]
    
    analysis = optimizer.analyze_prompt(initial_prompt, examples, 
                                      "Need detailed, structured summaries")
    
    optimized = optimizer.optimize_prompt(initial_prompt, analysis, examples)
    
    return {
        "original": initial_prompt,
        "analysis": analysis,
        "optimized": optimized
    }


if __name__ == "__main__":
    result = create_meta_prompting_example()
    print("Original Prompt:", result["original"])
    print("\nAnalysis:")
    print(f"- Clarity: {result['analysis'].clarity_score}/10")
    print(f"- Specificity: {result['analysis'].specificity_score}/10")
    print(f"- Issues: {result['analysis'].ambiguities}")
    print("\nOptimized Prompt:", result["optimized"].optimized_prompt)
    print("\nChanges Made:")
    for change in result["optimized"].changes_made:
        print(f"- {change}")